---
title: "Concrete Work!"
excerpt: "This study analyzed four neural network models (A-D) to assess the impact of data scaling, model depth, and training epochs on accuracy and efficiency. Each model was trained 50 times with random dataset splits to evaluate stability via mean squared error (MSE) and weight variability.

Model A (baseline: 1 layer, 50 epochs, no scaling).<br>
  
Model B (scaling added) showed improved accuracy and reduced weight variation, highlighting normalizationâ€™s critical role.<br>
  
Model C (100 epochs) achieved higher accuracy but with longer training times.<br>
  
Model D (3 layers, 50 epochs) demonstrated faster training and greater stability than shallower models.<br>
  
Key findings: Scaling enhances performance and consistency, deeper architectures reduce training duration and variability, while more epochs improve accuracy at a computational cost. The results emphasize balancing architectural complexity, normalization, and resource allocation for efficient model optimization."
image: "/assets/images/projects/model_study.png"
tags: 
  - "Artificial Intelligence |"
  - " Deep Learning |"
  - " A/B Test |"
  - " TensorFlow |"
  - " Python"
github: "/ArdaKaymaz/Model_Comparison_Study"
---
